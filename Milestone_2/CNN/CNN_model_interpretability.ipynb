{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPyHnnNyeOXY",
        "outputId": "3605f6bc-b783-420f-ad52-bcc26338e49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIKL1N16eRgY",
        "outputId": "c6f0f8d3-1022-4ce7-9404-419b659cb13e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 877 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=9cfe637049e29d4f8e1a413ec6e245fe9bf1552fb43a596b3cb2ede4c1cbd74f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z9bhjjkt/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy.data import Field, LabelField\n",
        "from torchtext.legacy.data import TabularDataset\n",
        "from torchtext.legacy.data import Iterator, BucketIterator"
      ],
      "metadata": {
        "id": "W13gPRUheTWe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "# from torchtext.data import Field, LabelField # For torch<=0.8.0, the importing of functions should be `from torchtext.data`\n",
        "# from torchtext.data import TabularDataset\n",
        "# from torchtext.data import Iterator, BucketIterator\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "\n",
        "manual_seed = 77\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed(manual_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DplLgH6eYBe",
        "outputId": "3dcbb137-ac31-4886-bf18-a57957a5c731"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = en_core_web_lg.load()\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
      ],
      "metadata": {
        "id": "k2swd5d8eaoo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
        "LABEL = Field(sequential=False, unk_token = None)\n",
        "\n",
        "train, val = TabularDataset.splits(\n",
        "               path=\"./drive/My Drive/CNN/\", # the root directory where the data lies\n",
        "               train='train_sentiment.tsv', validation=\"val_sentiment.tsv\", # file names\n",
        "               format='tsv',\n",
        "               skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "               fields=[('tweet', TEXT), ('label', LABEL)])"
      ],
      "metadata": {
        "id": "WskwKsDeebbr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = TabularDataset.splits(\n",
        "               path=\"/content/drive/MyDrive/CNN/\", # the root directory where the data lies\n",
        "               test=\"masks.tsv\", # file names\n",
        "               format='tsv',\n",
        "               skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "               fields=[(None, None), ('tweet', TEXT), (None, None), ('label', LABEL)])\n",
        "test = test[0]"
      ],
      "metadata": {
        "id": "U8xb8i67eeaJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "  \n",
        "# specifying the zip file name\n",
        "file_name = \"./drive/My Drive/CNN/emb.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    # printing all the contents of the zip file\n",
        "    zip.printdir()\n",
        "  \n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    zip.extractall()\n",
        "    print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo6UC6S7ekfJ",
        "outputId": "78817e00-602d-4b77-c94e-744660c49865"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name                                             Modified             Size\n",
            "glove.twitter.27B.100d.txt                     2015-12-22 16:04:54   1021671926\n",
            "glove.twitter.27B.200d.txt                     2015-12-22 16:04:54   2057595650\n",
            "glove.twitter.27B.25d.txt                      2015-12-22 16:04:54    257699930\n",
            "glove.twitter.27B.50d.txt                      2015-12-22 16:04:54    510889212\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext.vocab as vocab\n",
        "loaded_vectors = torchtext.vocab.Vectors('/content/drive/MyDrive/CNN/glove.twitter.27B.200d.txt')\n",
        "TEXT.build_vocab(train, vectors=loaded_vectors, max_size=len(loaded_vectors.stoi))\n",
        "TEXT.vocab.set_vectors(stoi=loaded_vectors.stoi, vectors=loaded_vectors.vectors, dim=loaded_vectors.dim)\n",
        "LABEL.build_vocab(train)\n",
        "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
        "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-xh6KslenFB",
        "outputId": "06b7f39e-3b44-4a61-b363-3d2cd73da18b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1193516/1193517 [01:25<00:00, 13887.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size of TEXT: 50220\n",
            "Vocabulary size of LABEL: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFN2ui-cepwI",
        "outputId": "5289408c-c52b-4e4b-83fd-6eba3d16c36a"
      },
      "execution_count": 521,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL.vocab.stoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RY7yXvsiZhm",
        "outputId": "4039f6c3-2689-40bd-d4c4-90e6c521fe89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(None, {'0': 2, '1': 0, '2': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test), \n",
        "    batch_size = 32,\n",
        "    sort_key=lambda x: len(x.tweet), \n",
        "    sort=True,\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "w78pWpixesds"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(CNN_BiLSTM, self).__init__()\n",
        "        self.args = args\n",
        "        self.hidden_dim = args.lstm_hidden_dim\n",
        "        self.num_layers = args.lstm_num_layers\n",
        "        V = args.embed_num\n",
        "        D = args.embed_dim\n",
        "        C = args.class_num\n",
        "        self.C = C\n",
        "        Ci = 1\n",
        "        Co = args.kernel_num\n",
        "        Ks = args.kernel_sizes\n",
        "        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n",
        "        # pretrained  embedding\n",
        "        if args.word_Embedding:\n",
        "            self.embed.weight.data.copy_(args.pretrained_weight)\n",
        "\n",
        "        # CNN\n",
        "        self.convs1 = [nn.Conv2d(Ci, Co, (K, D), padding=(K//2, 0), stride=1) for K in Ks]\n",
        "        print(self.convs1)\n",
        "        # for cnn cuda\n",
        "        # if self.args.cuda is True:\n",
        "        #     for conv in self.convs1:\n",
        "        #         conv = conv.cuda()\n",
        "\n",
        "        # BiLSTM\n",
        "        self.bilstm = nn.LSTM(D, self.hidden_dim, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True, bias=True)\n",
        "\n",
        "        # linear\n",
        "        L = len(Ks) * Co + self.hidden_dim * 2\n",
        "        self.hidden2label1 = nn.Linear(L, L // 2)\n",
        "        self.hidden2label2 = nn.Linear(L // 2, C)\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        embed = self.embed(x)\n",
        "\n",
        "        # CNN\n",
        "        cnn_x = embed\n",
        "        # print(cnn_x.shape)\n",
        "        cnn_x = torch.transpose(cnn_x, 0, 1)\n",
        "        # print(cnn_x.shape)\n",
        "        cnn_x = cnn_x.unsqueeze(1)\n",
        "        cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs1]  # [(N,Co,W), ...]*len(Ks)\n",
        "        cnn_x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in cnn_x]  # [(N,Co), ...]*len(Ks)\n",
        "        cnn_x = torch.cat(cnn_x, 1)\n",
        "        cnn_x = self.dropout(cnn_x)\n",
        "        # print(cnn_x.shape)\n",
        "        # BiLSTM\n",
        "        # print('blstm')\n",
        "        bilstm_x = embed.view(len(x), embed.size(1), -1)\n",
        "        # print(bilstm_x.shape)\n",
        "        bilstm_out, _ = self.bilstm(bilstm_x)\n",
        "        # print(bilstm_out.shape)\n",
        "        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n",
        "        # print(bilstm_out.shape)\n",
        "        bilstm_out = torch.transpose(bilstm_out, 1, 2)\n",
        "        # print(bilstm_out.shape)\n",
        "        bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2)).squeeze(2)\n",
        "        # print(bilstm_out.shape)\n",
        "        # bilstm_out = F.tanh(bilstm_out)\n",
        "\n",
        "        # CNN and BiLSTM CAT\n",
        "        cnn_x = torch.transpose(cnn_x, 0, 1)\n",
        "        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n",
        "        cnn_bilstm_out = torch.cat((cnn_x, bilstm_out), 0)\n",
        "        cnn_bilstm_out = torch.transpose(cnn_bilstm_out, 0, 1)\n",
        "\n",
        "        # linear\n",
        "        cnn_bilstm_out = self.hidden2label1(F.tanh(cnn_bilstm_out))\n",
        "        cnn_bilstm_out = self.hidden2label2(F.tanh(cnn_bilstm_out))\n",
        "\n",
        "        # output\n",
        "        logit = cnn_bilstm_out\n",
        "        return logit"
      ],
      "metadata": {
        "id": "3zY4S9D9evZF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randint(low=0, high=10, size=(7,32))\n",
        "print(x.shape)\n",
        "pred = model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyMZsnBOIk9x",
        "outputId": "1a89d54d-7a1a-490b-8da9-e24f874ecb8b"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token_tensor = torch.tensor(token_ids, device=device).unsqueeze(0)\n",
        "# S(model, token_tensor, 6, 1)"
      ],
      "metadata": {
        "id": "FEEXH4ZHFMBM"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(TEXT.vocab.stoi) \n",
        "hidden_dim = 700\n",
        "embedding_dim = 200 \n",
        "num_layers = 1\n",
        "kernel_sizes = [4,5,6] \n",
        "kernels_num = 32 \n",
        "dropout = 0.7\n",
        "output_size = 3\n",
        "lr = 0.001        \n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "num_epoch = 20 \n",
        "\n",
        "class CNNArgs:\n",
        "  def __init__(self, hidden_dim, num_layers, vocabulary_size, embedding_dim, output_size, kernels_num, kernel_sizes, dropout, padIdx) -> None:\n",
        "      self.lstm_hidden_dim = hidden_dim\n",
        "      self.lstm_num_layers = num_layers\n",
        "      self.embed_num = vocabulary_size \n",
        "      self.embed_dim = embedding_dim\n",
        "      self.class_num = output_size\n",
        "      self.kernel_num = kernels_num\n",
        "      self.kernel_sizes = kernel_sizes\n",
        "      self.dropout = dropout\n",
        "      self.paddingId = padIdx\n",
        "      self.word_Embedding = None\n",
        "      self.cuda = True\n",
        "\n",
        "\n",
        "args_for_cnn = CNNArgs(hidden_dim, num_layers, vocabulary_size, embedding_dim, output_size, kernels_num, kernel_sizes, dropout, PAD_IDX)\n",
        "\n",
        "model = CNN_BiLSTM(args_for_cnn).to(device)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(model)\n",
        "print(vocabulary_size)\n",
        "model.embed.weight.data.copy_(pretrained_embeddings)\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embed.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embed.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-dnKkY_eze5",
        "outputId": "c17cae9a-7079-4186-b67d-ff578cc55dfd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Conv2d(1, 32, kernel_size=(4, 200), stride=(1, 1), padding=(2, 0)), Conv2d(1, 32, kernel_size=(5, 200), stride=(1, 1), padding=(2, 0)), Conv2d(1, 32, kernel_size=(6, 200), stride=(1, 1), padding=(3, 0))]\n",
            "CNN_BiLSTM(\n",
            "  (embed): Embedding(50220, 200, padding_idx=1)\n",
            "  (bilstm): LSTM(200, 700, dropout=0.7, bidirectional=True)\n",
            "  (hidden2label1): Linear(in_features=1496, out_features=748, bias=True)\n",
            "  (hidden2label2): Linear(in_features=748, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "50220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        batch_input, labels = batch.tweet, batch.label\n",
        "        batch_input = batch_input.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_input)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.cpu().item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            batch_input, labels = batch.tweet, batch.label\n",
        "            batch_input = batch_input.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_input)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score"
      ],
      "metadata": {
        "id": "b5fz3y4dfe4V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCH = 15\n",
        "total_step = len(train_iter)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(MAX_EPOCH, desc=\"Epoch\"):\n",
        "    train_loss = train(model, train_iter, optimizer, criterion)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state_dict_model = model.state_dict() \n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': state_dict_model,\n",
        "        'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, MAX_EPOCH, train_loss, val_loss, val_acc, val_f1))\n",
        "    torch.save(state, \"./drive/My Drive/CNN/CNN_LSTM_F\"+str(epoch+1)+\".pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWXQ8rQ1flfI",
        "outputId": "e3675fef-4506-4ca7-9f5c-a00d0e2c2590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/15], Train Loss: 0.5173, Validation Loss: 0.7103, Validation Accuracy: 0.6865, Validation F1: 0.6682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   7%|▋         | 1/15 [34:35<8:04:12, 2075.18s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [2/15], Train Loss: 0.3025, Validation Loss: 0.9422, Validation Accuracy: 0.6560, Validation F1: 0.6429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  13%|█▎        | 2/15 [1:09:04<7:28:53, 2071.83s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [3/15], Train Loss: 0.1742, Validation Loss: 1.3201, Validation Accuracy: 0.6290, Validation F1: 0.6196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  20%|██        | 3/15 [1:43:53<6:55:55, 2079.59s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [4/15], Train Loss: 0.1139, Validation Loss: 1.6470, Validation Accuracy: 0.6485, Validation F1: 0.6230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  27%|██▋       | 4/15 [2:18:33<6:21:18, 2079.90s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [5/15], Train Loss: 0.0846, Validation Loss: 1.8795, Validation Accuracy: 0.6530, Validation F1: 0.6080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  33%|███▎      | 5/15 [2:53:15<5:46:45, 2080.60s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [6/15], Train Loss: 0.0766, Validation Loss: 1.9758, Validation Accuracy: 0.6435, Validation F1: 0.6192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 6/15 [3:27:58<5:12:11, 2081.30s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [7/15], Train Loss: 0.0522, Validation Loss: 2.2490, Validation Accuracy: 0.6425, Validation F1: 0.6274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  47%|████▋     | 7/15 [4:02:33<4:37:15, 2079.42s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [8/15], Train Loss: 0.0365, Validation Loss: 2.2763, Validation Accuracy: 0.6380, Validation F1: 0.6151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  53%|█████▎    | 8/15 [4:37:19<4:02:49, 2081.41s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [9/15], Train Loss: 0.0333, Validation Loss: 2.7409, Validation Accuracy: 0.6355, Validation F1: 0.5964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 9/15 [5:11:56<3:27:59, 2079.86s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [10/15], Train Loss: 0.0306, Validation Loss: 2.9121, Validation Accuracy: 0.6250, Validation F1: 0.6012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 10/15 [5:47:11<2:54:13, 2090.70s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [11/15], Train Loss: 0.0259, Validation Loss: 2.8866, Validation Accuracy: 0.6280, Validation F1: 0.5984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  73%|███████▎  | 11/15 [6:22:14<2:19:38, 2094.63s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [12/15], Train Loss: 0.0260, Validation Loss: 2.9893, Validation Accuracy: 0.6380, Validation F1: 0.6006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 12/15 [6:57:04<1:44:39, 2093.15s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [13/15], Train Loss: 0.0222, Validation Loss: 2.9984, Validation Accuracy: 0.6190, Validation F1: 0.6048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  87%|████████▋ | 13/15 [7:32:19<1:09:59, 2099.70s/it]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [14/15], Train Loss: 0.0180, Validation Loss: 3.2289, Validation Accuracy: 0.6365, Validation F1: 0.6209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  93%|█████████▎| 14/15 [8:07:08<34:56, 2096.59s/it]  /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [15/15], Train Loss: 0.0203, Validation Loss: 3.3065, Validation Accuracy: 0.6180, Validation F1: 0.6016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 15/15 [8:42:22<00:00, 2089.49s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args_for_cnn = CNNArgs(hidden_dim, num_layers, vocabulary_size, embedding_dim, output_size, kernels_num, kernel_sizes, dropout, PAD_IDX)\n",
        "\n",
        "model = CNN_BiLSTM(args_for_cnn).to(device)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(model)\n",
        "print(vocabulary_size)\n",
        "model.embed.weight.data.copy_(pretrained_embeddings)\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embed.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embed.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Egv9gEWlUD2",
        "outputId": "bc09f29e-d0a5-4826-b283-56d717e14a1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Conv2d(1, 32, kernel_size=(4, 200), stride=(1, 1), padding=(2, 0)), Conv2d(1, 32, kernel_size=(5, 200), stride=(1, 1), padding=(2, 0)), Conv2d(1, 32, kernel_size=(6, 200), stride=(1, 1), padding=(3, 0))]\n",
            "CNN_BiLSTM(\n",
            "  (embed): Embedding(50220, 200, padding_idx=1)\n",
            "  (bilstm): LSTM(200, 700, dropout=0.7, bidirectional=True)\n",
            "  (hidden2label1): Linear(in_features=1496, out_features=748, bias=True)\n",
            "  (hidden2label2): Linear(in_features=748, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "50220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/CNN/CNN_LSTM_F1.pt')['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNyOxQZulXcL",
        "outputId": "abc30602-8104-4dcb-e2e1-950776da9421"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# masks\n",
        "# 0.9200501243273417 0.6333333333333333 0.5588888888888889\n",
        "# test_loss, test_acc, test_f1 = evaluate(model, test_iter, criterion)\n",
        "# print(test_loss, test_acc, test_f1)\n",
        "# vaccines\n",
        "# 0.8588742166757584 0.5841584158415841 0.6099456099456099\n",
        "test_loss, test_acc, test_f1 = evaluate(model, test_iter, criterion)\n",
        "print(test_loss, test_acc, test_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THhOYqOdf4J8",
        "outputId": "77a737e0-1bb2-4536-e3db-156c127f3832"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.917639414469401 0.6222222222222222 0.5515091093908828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgmAUGRql6CB",
        "outputId": "e93e0346-2325-421d-815e-caa92d6a9ef5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_BiLSTM(\n",
            "  (embed): Embedding(50220, 200, padding_idx=1)\n",
            "  (bilstm): LSTM(200, 700, dropout=0.7, bidirectional=True)\n",
            "  (hidden2label1): Linear(in_features=1496, out_features=748, bias=True)\n",
            "  (hidden2label2): Linear(in_features=748, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL.vocab.stoi\n",
        "# \"negative\" - 0\n",
        "# \"neutral\" - 1\n",
        "# \"positive\" - 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPqh9aEytg6w",
        "outputId": "e7937b42-416d-4bc6-d6c2-c550d868d3a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(None, {'0': 2, '1': 0, '2': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# sample_sentence = 'it was an amazing performance'\n",
        "# print(sample_sentence)\n",
        "# token_ids = [TEXT.vocab.stoi[tok] for tok in tokenize_en(sample_sentence)]\n",
        "# print(token_ids)\n",
        "# if len(token_ids) < 7:\n",
        "#     token_ids += [TEXT.vocab.stoi['pad']] * (7 - len(token_ids))\n",
        "# print(token_ids)\n",
        "# token_tensor = torch.tensor(token_ids, device=device).unsqueeze(0).transpose(1,0)\n",
        "# print(token_tensor)\n",
        "# print(token_tensor.size())\n",
        "# probabilities, predicted = torch.max(model(token_tensor).cpu().data, 1)\n",
        "# print(probabilities)\n",
        "# print(predicted)"
      ],
      "metadata": {
        "id": "Ndg0QHE7l7zt"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction = model(token_tensor).data\n",
        "# prediction"
      ],
      "metadata": {
        "id": "qdcYyBBFvPol"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def S(trained_model, token_tensor, erase_tokenid, gold_label_id):\n",
        "    '''\n",
        "    input:\n",
        "    i) trained_model - trained CNN model on IMDB dataset\n",
        "    ii) token_tensor - tensor containing token ids for a test sample 'e'\n",
        "    iii) erase_tokenid - id corresponding to the token to be erased. -1 if no token needs to be erased (useful in computing S(e,c))\n",
        "    iv) gold_label_id - gold sentiment label for the test sample 'e'\n",
        "    \n",
        "    returns:\n",
        "    probability by model to the gold label. S(e,c) if erase_tokenid == -1 else S(e,c,~d)\n",
        "    '''\n",
        "    \n",
        "    # print(token_tensor.shape)\n",
        "    #token_tensor = [1, sent len]  note: batch_size is 1\n",
        "    embedded = model.embed(token_tensor)\n",
        "    # print(embedded)\n",
        "    # print(embedded.shape)\n",
        "    #embedded = [batch size, sent len, emb dim]\n",
        "    \n",
        "    # check if word embedings corresponding to a token has to be erased\n",
        "    if erase_tokenid != -1:\n",
        "        embedded[0, erase_tokenid, :].fill_(0.0)\n",
        "    # print(embedded)\n",
        "    # print(embedded.shape)\n",
        "    #embedded = [batch size, sent len, emb dim]\n",
        "    # cnn_x = torch.transpose(cnn_x, 0, 1)\n",
        "    # print(0)\n",
        "        \n",
        "    cnn_x = embedded\n",
        "    # print(cnn_x.shape)\n",
        "    # print(1)\n",
        "    # cnn_x = torch.transpose(cnn_x, 0, 1)\n",
        "    # print(cnn_x.shape)\n",
        "    cnn_x = cnn_x.unsqueeze(1)\n",
        "    cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in model.convs1]  # [(N,Co,W), ...]*len(Ks)\n",
        "    cnn_x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in cnn_x]  # [(N,Co), ...]*len(Ks)\n",
        "    cnn_x = torch.cat(cnn_x, 1)\n",
        "    cnn_x = model.dropout(cnn_x)\n",
        "    # print('cnn output:')\n",
        "    # print(cnn_x.shape)\n",
        "    # print('embedded blstm')\n",
        "    bilstm_x = embedded.view(embedded.size(1), len(token_tensor), -1)\n",
        "\n",
        "    # print(bilstm_x.shape)\n",
        "    bilstm_out, _ = model.bilstm(bilstm_x)\n",
        "    # print(bilstm_out.shape)\n",
        "    bilstm_out = torch.transpose(bilstm_out, 0, 1)\n",
        "    bilstm_out = torch.transpose(bilstm_out, 1, 2)\n",
        "    bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2)).squeeze(2)\n",
        "        # bilstm_out = F.tanh(bilstm_out)\n",
        "    # print(bilstm_out.shape)\n",
        "        # CNN and BiLSTM CAT\n",
        "    cnn_x = torch.transpose(cnn_x, 0, 1)\n",
        "    bilstm_out = torch.transpose(bilstm_out, 0, 1)\n",
        "    cnn_bilstm_out = torch.cat((cnn_x, bilstm_out), 0)\n",
        "    cnn_bilstm_out = torch.transpose(cnn_bilstm_out, 0, 1)\n",
        "\n",
        "        # linear\n",
        "    cnn_bilstm_out = model.hidden2label1(F.tanh(cnn_bilstm_out))\n",
        "    cnn_bilstm_out = model.hidden2label2(F.tanh(cnn_bilstm_out))\n",
        "\n",
        "        # output\n",
        "    logit = F.softmax(cnn_bilstm_out)\n",
        "    probabilities, predicted = torch.max(logit, 1)\n",
        "\n",
        "    return probabilities.item()"
      ],
      "metadata": {
        "id": "zI9h2S-ZD0N0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "def interpret_sentence(sentence, min_len = 7, true_label = 1):\n",
        "  '''\n",
        "  input:\n",
        "  i) sentence - the sample sentence to be analyzed\n",
        "  ii) min_len - minimum length of the sentence (alternatively, maximum width of the filter used by the orginal author)\n",
        "  iii) true_label - gold sentiment label for the sample sentence to be analyzed\n",
        "  returns:\n",
        "  None (actually the code plots the heatmap and returns nothing)\n",
        "  '''\n",
        "  # tokenize sentence\n",
        "  token_ids = [TEXT.vocab.stoi[tok] for tok in tokenize_en(sentence)]\n",
        "  # pad the sentence if necessary\n",
        "  if len(token_ids) < 7: # where 5 in min_len\n",
        "    token_ids += [TEXT.vocab.stoi['pad']] * (7 - len(token_ids))\n",
        "  \n",
        "  # convert to tensor\n",
        "  token_tensor = torch.tensor(token_ids, device=device).unsqueeze(0)\n",
        "    \n",
        "  # get prediction\n",
        "  probabilities, prediction = torch.max(model(token_tensor.transpose(1,0)).cpu().data, 1)\n",
        "  prediction = prediction.item()\n",
        "  # print('pred')\n",
        "  # print(prediction)\n",
        "   \n",
        "  # get word importance\n",
        "  tokens, imp_scores = [None], []\n",
        "  s_ec = S(model, token_tensor, -1, true_label)\n",
        "  # print(\"ses\")\n",
        "  # print(s_ec)\n",
        "  for t_i, token_id in enumerate(token_ids):\n",
        "    if token_id != TEXT.vocab.stoi['pad']: # we don't need to calculate importance score for pad tokens (seriously?)\n",
        "      s_e = S(model, token_tensor, t_i, true_label)\n",
        "      tokens.append(TEXT.vocab.itos[token_id])\n",
        "      # print((s_ec-s_e)/s_ec)\n",
        "      imp_scores.append([((s_ec-s_e)/s_ec)])\n",
        "  \n",
        "  # plot the heatmap and other values\n",
        "  # print(tokens)\n",
        "  # print(imp_scores)\n",
        "  print('sentence = %s'%sentence)\n",
        "  print('predicted label = %d; gold label = %d'%(prediction, true_label))\n",
        "  # print(imp_scores)\n",
        "  # fig, ax = plt.subplots()\n",
        "  # im = ax.imshow(np.array(imp_scores).transpose(), cmap=plt.cm.Blues)\n",
        "  # ax.set_xticklabels(tokens)\n",
        "  # ax.set_yticklabels(['importance'])\n",
        "  # plt.colorbar(im)\n",
        "  # fig.set_size_inches(15, 8)\n",
        "  # fig.tight_layout()\n",
        "  # plt.show()\n",
        "  lst = [score[0] for score in imp_scores]\n",
        "  source = pd.DataFrame({'tokens': tokens[1:], 'score': lst})\n",
        "  return source, tokens"
      ],
      "metadata": {
        "id": "wHzT7ZeVJjIV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualisation(df, tokens, w=500):\n",
        "  viz = alt.Chart(df).mark_rect().encode(\n",
        "    x=alt.X('tokens', sort=tokens[1:], axis=alt.Axis(labelAngle=-45)),\n",
        "    color=alt.Color('score', scale=alt.Scale(scheme='blues'))).properties(\n",
        "    width=w,\n",
        "    height=70\n",
        ")\n",
        "  return viz"
      ],
      "metadata": {
        "id": "DeTOnXbiCOVb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, tokens = interpret_sentence('it was an amazing performance', true_label = 1.0)\n",
        "visualisation(df, tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "JfMN8Jo7Kgnn",
        "outputId": "ad794f5c-81da-451a-ab92-1c3e4bdb7315"
      },
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = it was an amazing performance\n",
            "predicted label = 1; gold label = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-bb811be6133c46d7a7cb6bc791724a4c\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-bb811be6133c46d7a7cb6bc791724a4c\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-bb811be6133c46d7a7cb6bc791724a4c\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-519dc1c36e6c9524b2d0f8fe06d38c69\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"it\", \"was\", \"an\", \"amazing\", \"performance\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-519dc1c36e6c9524b2d0f8fe06d38c69\": [{\"tokens\": \"it\", \"score\": 0.0019215376142548654}, {\"tokens\": \"was\", \"score\": 0.0014937956100320687}, {\"tokens\": \"an\", \"score\": 0.0013206219246289295}, {\"tokens\": \"amazing\", \"score\": 0.4171611820829763}, {\"tokens\": \"performance\", \"score\": 0.006000933397160492}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 434
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df, tokens = interpret_sentence('368 people died yesterday. RIP those ignored through the boredom of repeated mistakes.', true_label = 2.0)\n",
        "print(df)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "_LKN-PxLIO7M",
        "outputId": "a75b4ef1-7dc1-41d9-98f0-11f1c98142fd"
      },
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = 368 people died yesterday. RIP those ignored through the boredom of repeated mistakes.\n",
            "predicted label = 2; gold label = 2\n",
            "       tokens     score\n",
            "0       <unk>  0.000000\n",
            "1      people -0.005417\n",
            "2        died  0.040391\n",
            "3   yesterday -0.001702\n",
            "4           . -0.000633\n",
            "5       <unk>  0.000000\n",
            "6       those -0.012151\n",
            "7     ignored  0.027310\n",
            "8     through  0.003373\n",
            "9         the  0.001165\n",
            "10    boredom  0.005314\n",
            "11         of -0.002616\n",
            "12   repeated -0.021318\n",
            "13   mistakes -0.001154\n",
            "14          .  0.009348\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-de92a7d983ca4bafba00afd2a7289004\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-de92a7d983ca4bafba00afd2a7289004\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-de92a7d983ca4bafba00afd2a7289004\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-d78b2580c9b1bd756515aaafac672520\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"<unk>\", \"people\", \"died\", \"yesterday\", \".\", \"<unk>\", \"those\", \"ignored\", \"through\", \"the\", \"boredom\", \"of\", \"repeated\", \"mistakes\", \".\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-d78b2580c9b1bd756515aaafac672520\": [{\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"people\", \"score\": -0.0054168282190583845}, {\"tokens\": \"died\", \"score\": 0.04039050947776877}, {\"tokens\": \"yesterday\", \"score\": -0.0017016140556470947}, {\"tokens\": \".\", \"score\": -0.0006325512404918842}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"those\", \"score\": -0.012151450181339892}, {\"tokens\": \"ignored\", \"score\": 0.02730969555078311}, {\"tokens\": \"through\", \"score\": 0.0033730730875824815}, {\"tokens\": \"the\", \"score\": 0.001165354016136971}, {\"tokens\": \"boredom\", \"score\": 0.005313558466941643}, {\"tokens\": \"of\", \"score\": -0.002616124371354187}, {\"tokens\": \"repeated\", \"score\": -0.02131844934288938}, {\"tokens\": \"mistakes\", \"score\": -0.0011535737096338835}, {\"tokens\": \".\", \"score\": 0.009347993327224647}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 436
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df, tokens = interpret_sentence(' \\n\\nWhat have we become when our boredom means that 368 people die and no one cares. \\n\\nBut we are free of the heinous mask. \\n\\nThe UK is full of selfishness. #CovidIsntOver', true_label = 2.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "Ao-M1QvMPbM9",
        "outputId": "7ebc7c23-ec6c-4300-854f-3cf1280941f2"
      },
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence =  \n",
            "\n",
            "What have we become when our boredom means that 368 people die and no one cares. \n",
            "\n",
            "But we are free of the heinous mask. \n",
            "\n",
            "The UK is full of selfishness. #CovidIsntOver\n",
            "predicted label = 2; gold label = 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-4e6a313bc60142f6ba5bf1a40d800c67\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-4e6a313bc60142f6ba5bf1a40d800c67\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-4e6a313bc60142f6ba5bf1a40d800c67\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-7c3ee691e8c22f1d86f7149106006d8d\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"<unk>\", \"<unk>\", \"have\", \"we\", \"become\", \"when\", \"our\", \"boredom\", \"means\", \"that\", \"<unk>\", \"people\", \"die\", \"and\", \"no\", \"one\", \"cares\", \".\", \"<unk>\", \"<unk>\", \"we\", \"are\", \"free\", \"of\", \"the\", \"heinous\", \"mask\", \".\", \"<unk>\", \"<unk>\", \"<unk>\", \"is\", \"full\", \"of\", \"selfishness\", \".\", \"#\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-7c3ee691e8c22f1d86f7149106006d8d\": [{\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"have\", \"score\": 0.0009283068683253271}, {\"tokens\": \"we\", \"score\": -0.008583861530061215}, {\"tokens\": \"become\", \"score\": -0.005008870495043712}, {\"tokens\": \"when\", \"score\": 0.003023792652590927}, {\"tokens\": \"our\", \"score\": -0.003999407964916938}, {\"tokens\": \"boredom\", \"score\": 0.0012392094195965812}, {\"tokens\": \"means\", \"score\": -0.006013220347911863}, {\"tokens\": \"that\", \"score\": -0.0019440469895164877}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"people\", \"score\": 0.008317614008010853}, {\"tokens\": \"die\", \"score\": 0.10189727570021513}, {\"tokens\": \"and\", \"score\": 0.004539927970790693}, {\"tokens\": \"no\", \"score\": -0.003285315801984586}, {\"tokens\": \"one\", \"score\": 0.007580741308432481}, {\"tokens\": \"cares\", \"score\": 0.001399320350455258}, {\"tokens\": \".\", \"score\": 0.003003600813291045}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"we\", \"score\": -0.007471951302461003}, {\"tokens\": \"are\", \"score\": -0.002739747836285861}, {\"tokens\": \"free\", \"score\": -0.01412276780672682}, {\"tokens\": \"of\", \"score\": 0.00033737198163552216}, {\"tokens\": \"the\", \"score\": -0.0035145449519851653}, {\"tokens\": \"heinous\", \"score\": 0.1271735754576477}, {\"tokens\": \"mask\", \"score\": -0.012036989637509241}, {\"tokens\": \".\", \"score\": -0.0041454752126728125}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"is\", \"score\": 0.002604682552251076}, {\"tokens\": \"full\", \"score\": 0.0021792301434131857}, {\"tokens\": \"of\", \"score\": -0.00012988788934250776}, {\"tokens\": \"selfishness\", \"score\": 0.0013803581423947923}, {\"tokens\": \".\", \"score\": -0.01000123804450579}, {\"tokens\": \"#\", \"score\": -0.022405628552865764}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 437
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrong prediction\n",
        "df, tokens = interpret_sentence(\"Which science are we following now? Today it's $cience.  #covid #vaccines #masks #money #admityouwerewrong #freedom https://t.co/1QihIkgTn6\", true_label = 2.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9DPVyHBpPvmX",
        "outputId": "03c5e2fd-7646-4c52-fb1f-81d6dcedb7b7"
      },
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Which science are we following now? Today it's $cience.  #covid #vaccines #masks #money #admityouwerewrong #freedom https://t.co/1QihIkgTn6\n",
            "predicted label = 0; gold label = 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-bc8be1960ee044c7ab2f340482f312b8\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-bc8be1960ee044c7ab2f340482f312b8\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-bc8be1960ee044c7ab2f340482f312b8\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-db87ba33b9bd0d99449a0864de60c970\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"<unk>\", \"science\", \"are\", \"we\", \"following\", \"now\", \"?\", \"<unk>\", \"it\", \"'s\", \"$\", \"<unk>\", \".\", \" \", \"#\", \"<unk>\", \"#\", \"vaccines\", \"#\", \"masks\", \"#\", \"money\", \"#\", \"<unk>\", \"#\", \"freedom\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-db87ba33b9bd0d99449a0864de60c970\": [{\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"science\", \"score\": -0.02447233497852417}, {\"tokens\": \"are\", \"score\": -0.03140458135727861}, {\"tokens\": \"we\", \"score\": -0.06663015226505473}, {\"tokens\": \"following\", \"score\": 0.034692409575148554}, {\"tokens\": \"now\", \"score\": 0.009347559139924974}, {\"tokens\": \"?\", \"score\": 0.04524754849201905}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"it\", \"score\": -0.018389852775464774}, {\"tokens\": \"'s\", \"score\": 0.014755699502306152}, {\"tokens\": \"$\", \"score\": -0.00030664436990059145}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \".\", \"score\": -0.03666280839667816}, {\"tokens\": \" \", \"score\": 0.01863643989180502}, {\"tokens\": \"#\", \"score\": -0.05993499415131211}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.01816885127479947}, {\"tokens\": \"vaccines\", \"score\": 0.030681902619921084}, {\"tokens\": \"#\", \"score\": 0.017817240568368364}, {\"tokens\": \"masks\", \"score\": 0.03913289276026661}, {\"tokens\": \"#\", \"score\": -0.01760650395542894}, {\"tokens\": \"money\", \"score\": 0.014653893265084598}, {\"tokens\": \"#\", \"score\": -0.018569794725226574}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.012936516265633845}, {\"tokens\": \"freedom\", \"score\": 0.19374255508376226}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 438
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"negative\" - 0 - 2\n",
        "# \"neutral\" - 1 - 0\n",
        "# \"positive\" - 2 - 1"
      ],
      "metadata": {
        "id": "5CpjhjZKMyGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrong prediction\n",
        "df, tokens = interpret_sentence(\"#votefordout #masks #covid #Dougford #ontario I wish I lived in PEI or Quebec where the premier listened to science https://t.co/uU757dQ1Rw\", true_label = 2.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "Val1b_6MQGb2",
        "outputId": "1937342d-fba6-4bfd-8194-073b8790920b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = #votefordout #masks #covid #Dougford #ontario I wish I lived in PEI or Quebec where the premier listened to science https://t.co/uU757dQ1Rw\n",
            "predicted label = 1; gold label = 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-80e9688515d0434392704b120a95a2f8\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-80e9688515d0434392704b120a95a2f8\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-80e9688515d0434392704b120a95a2f8\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-4bbd528bd46a5b8ecd842a7ee63d1b00\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"#\", \"<unk>\", \"#\", \"masks\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"ontario\", \"i\", \"wish\", \"i\", \"lived\", \"in\", \"pei\", \"or\", \"quebec\", \"where\", \"the\", \"premier\", \"listened\", \"to\", \"science\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-4bbd528bd46a5b8ecd842a7ee63d1b00\": [{\"tokens\": \"#\", \"score\": 0.06505134143174804}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.04463562023766731}, {\"tokens\": \"masks\", \"score\": -0.06945348973440811}, {\"tokens\": \"#\", \"score\": 0.05243338647073887}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.13826410135479428}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.02274852448395609}, {\"tokens\": \"ontario\", \"score\": 0.049350425654768666}, {\"tokens\": \"i\", \"score\": 0.11613041264551828}, {\"tokens\": \"wish\", \"score\": -0.07181363680001182}, {\"tokens\": \"i\", \"score\": 0.0478470169637192}, {\"tokens\": \"lived\", \"score\": -0.0881114252873242}, {\"tokens\": \"in\", \"score\": 0.038931848333456746}, {\"tokens\": \"pei\", \"score\": -0.07564356170841516}, {\"tokens\": \"or\", \"score\": -0.1736346723866386}, {\"tokens\": \"quebec\", \"score\": -0.024173294871086073}, {\"tokens\": \"where\", \"score\": -0.011002576502196184}, {\"tokens\": \"the\", \"score\": -0.00017973043827282343}, {\"tokens\": \"premier\", \"score\": 0.10288604571280532}, {\"tokens\": \"listened\", \"score\": -0.0035409191620190566}, {\"tokens\": \"to\", \"score\": -0.003680132910709}, {\"tokens\": \"science\", \"score\": -0.019007317154466984}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df, tokens = interpret_sentence(\"Every single conference I have gone to, I come home very sick. That is until now! I just spent many days around hundreds of people (all who were required to mask throughout the conference) and I am not sick!  #MasksWork #RetreatMigraine2022 #ConferencesDoneRight @CoalitionCHAMP\", true_label = 1.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "splk6TujQZBr",
        "outputId": "00fbcbc5-f7af-47c3-d2f0-1e184a919f1d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Every single conference I have gone to, I come home very sick. That is until now! I just spent many days around hundreds of people (all who were required to mask throughout the conference) and I am not sick!  #MasksWork #RetreatMigraine2022 #ConferencesDoneRight @CoalitionCHAMP\n",
            "predicted label = 2; gold label = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-74fe096ce1e94b738e4292e5b473084b\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-74fe096ce1e94b738e4292e5b473084b\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-74fe096ce1e94b738e4292e5b473084b\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-52b4dc8f51821419bb3fd0c33d60ce17\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"every\", \"single\", \"conference\", \"i\", \"have\", \"gone\", \"to\", \",\", \"i\", \"come\", \"home\", \"very\", \"sick\", \".\", \"that\", \"is\", \"until\", \"now\", \"!\", \"i\", \"just\", \"spent\", \"many\", \"days\", \"around\", \"hundreds\", \"of\", \"people\", \"(\", \"all\", \"who\", \"were\", \"required\", \"to\", \"mask\", \"throughout\", \"the\", \"conference\", \")\", \"and\", \"i\", \"am\", \"not\", \"sick\", \"!\", \" \", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-52b4dc8f51821419bb3fd0c33d60ce17\": [{\"tokens\": \"every\", \"score\": -0.015566263838276041}, {\"tokens\": \"single\", \"score\": 0.003633304499890502}, {\"tokens\": \"conference\", \"score\": -0.02152769657829507}, {\"tokens\": \"i\", \"score\": -0.0033988847336287307}, {\"tokens\": \"have\", \"score\": 0.017587458033774536}, {\"tokens\": \"gone\", \"score\": 0.0461954713621896}, {\"tokens\": \"to\", \"score\": 0.0024484470316304377}, {\"tokens\": \",\", \"score\": -0.03381700949177988}, {\"tokens\": \"i\", \"score\": -0.02316847344039391}, {\"tokens\": \"come\", \"score\": -0.048838608733626145}, {\"tokens\": \"home\", \"score\": 0.007718087095946888}, {\"tokens\": \"very\", \"score\": 0.07008464628859326}, {\"tokens\": \"sick\", \"score\": 0.3064030269300911}, {\"tokens\": \".\", \"score\": -0.03988156108867673}, {\"tokens\": \"that\", \"score\": -0.0038488285633257987}, {\"tokens\": \"is\", \"score\": 0.0021228595367742678}, {\"tokens\": \"until\", \"score\": -0.002302449464492552}, {\"tokens\": \"now\", \"score\": 0.0845777311073657}, {\"tokens\": \"!\", \"score\": -0.03430086868552447}, {\"tokens\": \"i\", \"score\": -0.02072091466829805}, {\"tokens\": \"just\", \"score\": -0.012539882853102546}, {\"tokens\": \"spent\", \"score\": 0.026989281452947114}, {\"tokens\": \"many\", \"score\": -0.043944137196368666}, {\"tokens\": \"days\", \"score\": -0.04695953606366011}, {\"tokens\": \"around\", \"score\": -0.08387301829297836}, {\"tokens\": \"hundreds\", \"score\": 0.13327171503858923}, {\"tokens\": \"of\", \"score\": -0.01759416035571726}, {\"tokens\": \"people\", \"score\": 0.030630741790382377}, {\"tokens\": \"(\", \"score\": -0.04994198857730539}, {\"tokens\": \"all\", \"score\": 0.00880854680094136}, {\"tokens\": \"who\", \"score\": -0.007513706652127204}, {\"tokens\": \"were\", \"score\": -0.029742078501470634}, {\"tokens\": \"required\", \"score\": 0.1949780551444439}, {\"tokens\": \"to\", \"score\": 0.03701652033532928}, {\"tokens\": \"mask\", \"score\": -0.06416076270162684}, {\"tokens\": \"throughout\", \"score\": -0.04386968489719768}, {\"tokens\": \"the\", \"score\": -0.021510496643670975}, {\"tokens\": \"conference\", \"score\": -0.028777105643451976}, {\"tokens\": \")\", \"score\": -0.044990264675501025}, {\"tokens\": \"and\", \"score\": -0.006944009286995687}, {\"tokens\": \"i\", \"score\": 0.009832387040842496}, {\"tokens\": \"am\", \"score\": 0.004578654897281667}, {\"tokens\": \"not\", \"score\": 0.1579585470270438}, {\"tokens\": \"sick\", \"score\": 0.08900828891497321}, {\"tokens\": \"!\", \"score\": 0.06583715069597557}, {\"tokens\": \" \", \"score\": -0.0017485792692498762}, {\"tokens\": \"#\", \"score\": -0.03615030578738232}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.05372823521887684}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.07297681933317872}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"Went to the cinema for the first time in over two years last night. The cinema wanted Covid passes and mask wearing. Felt safe snd had a great time #MasksWork  #CovidIsNotOver\"\n",
        "df, tokens = interpret_sentence(tweet, true_label = 1.0)\n",
        "print(df)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "atiDYj19QrR4",
        "outputId": "8ddc46c6-0a4e-4099-e015-d5b3b858b016"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Went to the cinema for the first time in over two years last night. The cinema wanted Covid passes and mask wearing. Felt safe snd had a great time #MasksWork  #CovidIsNotOver\n",
            "predicted label = 1; gold label = 1\n",
            "     tokens     score\n",
            "0      went  0.003533\n",
            "1        to  0.010863\n",
            "2       the  0.014964\n",
            "3    cinema  0.004385\n",
            "4       for -0.000398\n",
            "5       the -0.001119\n",
            "6     first -0.002968\n",
            "7      time -0.006005\n",
            "8        in -0.001713\n",
            "9      over -0.003675\n",
            "10      two -0.012342\n",
            "11    years -0.004103\n",
            "12     last -0.003900\n",
            "13    night -0.002500\n",
            "14        . -0.001869\n",
            "15      the  0.002625\n",
            "16   cinema -0.000662\n",
            "17   wanted  0.004856\n",
            "18    <unk>  0.000000\n",
            "19   passes  0.000435\n",
            "20      and -0.001705\n",
            "21     mask -0.007450\n",
            "22  wearing -0.007644\n",
            "23        .  0.016414\n",
            "24     felt -0.012062\n",
            "25     safe  0.019762\n",
            "26      snd -0.034972\n",
            "27      had  0.017694\n",
            "28        a  0.035144\n",
            "29    great  0.271539\n",
            "30     time  0.034871\n",
            "31        #  0.018155\n",
            "32    <unk>  0.000000\n",
            "33          -0.006120\n",
            "34        #  0.031301\n",
            "35    <unk>  0.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-fd684a9d9c5f404986179adb4e06bd56\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-fd684a9d9c5f404986179adb4e06bd56\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-fd684a9d9c5f404986179adb4e06bd56\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-70c93dd709a19fec84f5353e261f01d4\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"went\", \"to\", \"the\", \"cinema\", \"for\", \"the\", \"first\", \"time\", \"in\", \"over\", \"two\", \"years\", \"last\", \"night\", \".\", \"the\", \"cinema\", \"wanted\", \"<unk>\", \"passes\", \"and\", \"mask\", \"wearing\", \".\", \"felt\", \"safe\", \"snd\", \"had\", \"a\", \"great\", \"time\", \"#\", \"<unk>\", \" \", \"#\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-70c93dd709a19fec84f5353e261f01d4\": [{\"tokens\": \"went\", \"score\": 0.003532735772478522}, {\"tokens\": \"to\", \"score\": 0.01086317879227227}, {\"tokens\": \"the\", \"score\": 0.014963980562328642}, {\"tokens\": \"cinema\", \"score\": 0.0043845415146306485}, {\"tokens\": \"for\", \"score\": -0.00039823922349412007}, {\"tokens\": \"the\", \"score\": -0.001119123250706026}, {\"tokens\": \"first\", \"score\": -0.0029680584902699557}, {\"tokens\": \"time\", \"score\": -0.0060051946399906995}, {\"tokens\": \"in\", \"score\": -0.0017130607867762943}, {\"tokens\": \"over\", \"score\": -0.00367532248840147}, {\"tokens\": \"two\", \"score\": -0.012342092380551693}, {\"tokens\": \"years\", \"score\": -0.004102887133360547}, {\"tokens\": \"last\", \"score\": -0.0038999552168230427}, {\"tokens\": \"night\", \"score\": -0.0025003506017055145}, {\"tokens\": \".\", \"score\": -0.0018692675317796333}, {\"tokens\": \"the\", \"score\": 0.002624820723923444}, {\"tokens\": \"cinema\", \"score\": -0.0006619073462657139}, {\"tokens\": \"wanted\", \"score\": 0.004856289794596928}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"passes\", \"score\": 0.00043512408693671076}, {\"tokens\": \"and\", \"score\": -0.0017047845011628508}, {\"tokens\": \"mask\", \"score\": -0.007449895236561007}, {\"tokens\": \"wearing\", \"score\": -0.007644029526659024}, {\"tokens\": \".\", \"score\": 0.016413764231952874}, {\"tokens\": \"felt\", \"score\": -0.01206206718936298}, {\"tokens\": \"safe\", \"score\": 0.019762466692837977}, {\"tokens\": \"snd\", \"score\": -0.0349715426110104}, {\"tokens\": \"had\", \"score\": 0.01769424246831942}, {\"tokens\": \"a\", \"score\": 0.03514378058641459}, {\"tokens\": \"great\", \"score\": 0.27153893555758185}, {\"tokens\": \"time\", \"score\": 0.03487105416679048}, {\"tokens\": \"#\", \"score\": 0.01815510775854204}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \" \", \"score\": -0.006120280627339843}, {\"tokens\": \"#\", \"score\": 0.031301107692853096}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"75% of Torontonians are helplessly, hopelessly brainwashing. Please realize how preposterous your obedience to this nonsense is. #CanadaHasFallen #canada #masks #covid #covid19 #insanity https://t.co/3BHoZYZ70t\"\n",
        "df, tokens = interpret_sentence(tweet, true_label = 2.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "HEsHM_Y4RE8b",
        "outputId": "27300882-6cd4-4893-fe0d-53c4699dbbf4"
      },
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = 75% of Torontonians are helplessly, hopelessly brainwashing. Please realize how preposterous your obedience to this nonsense is. #CanadaHasFallen #canada #masks #covid #covid19 #insanity https://t.co/3BHoZYZ70t\n",
            "predicted label = 2; gold label = 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-12b2b6c5db6c4e7ebe421e209cafd609\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-12b2b6c5db6c4e7ebe421e209cafd609\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-12b2b6c5db6c4e7ebe421e209cafd609\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ca272a038cb10ab3b7ac8a6aecb0f9eb\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"75\", \"%\", \"of\", \"<unk>\", \"are\", \"<unk>\", \",\", \"<unk>\", \"brainwashing\", \".\", \"<unk>\", \"realize\", \"how\", \"preposterous\", \"your\", \"obedience\", \"to\", \"this\", \"nonsense\", \"is\", \".\", \"#\", \"<unk>\", \"#\", \"canada\", \"#\", \"masks\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"insanity\", \"<unk>\", \"t\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-ca272a038cb10ab3b7ac8a6aecb0f9eb\": [{\"tokens\": \"75\", \"score\": -0.1309764148822974}, {\"tokens\": \"%\", \"score\": -0.37700415737708187}, {\"tokens\": \"of\", \"score\": -0.09429919474911568}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"are\", \"score\": 0.0327949574886221}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \",\", \"score\": -0.10683501922717217}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"brainwashing\", \"score\": 0.07957834842855037}, {\"tokens\": \".\", \"score\": -0.06549502748001225}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"realize\", \"score\": -0.10190798593340297}, {\"tokens\": \"how\", \"score\": -0.01326657681611408}, {\"tokens\": \"preposterous\", \"score\": 0.2544261162654131}, {\"tokens\": \"your\", \"score\": -0.0757477401652964}, {\"tokens\": \"obedience\", \"score\": -0.20565417990693197}, {\"tokens\": \"to\", \"score\": -0.05087725270331515}, {\"tokens\": \"this\", \"score\": 0.0993170235167987}, {\"tokens\": \"nonsense\", \"score\": -0.06547094977758963}, {\"tokens\": \"is\", \"score\": 0.009063950259025077}, {\"tokens\": \".\", \"score\": -0.0009306091291023868}, {\"tokens\": \"#\", \"score\": -0.060202084275042325}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.07938560819979797}, {\"tokens\": \"canada\", \"score\": -0.05240031564325954}, {\"tokens\": \"#\", \"score\": -0.09456808219439673}, {\"tokens\": \"masks\", \"score\": -0.11660238236411488}, {\"tokens\": \"#\", \"score\": -0.09705994648107938}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.07021046165495419}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.03480686895336837}, {\"tokens\": \"insanity\", \"score\": 0.036452573983485066}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"t\", \"score\": -0.24571354626957514}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 416
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"Good morning friends!  I have a question. No judgement, I promise.  Are you still wearing your mask when indoors shopping, or at the mall?  #Covid #Masks\"\n",
        "df, tokens = interpret_sentence(tweet, true_label = 0.0)\n",
        "print(df)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "gnfMEo3WRR29",
        "outputId": "3e3d34aa-8676-4418-86bd-c96ece10aaa3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Good morning friends!  I have a question. No judgement, I promise.  Are you still wearing your mask when indoors shopping, or at the mall?  #Covid #Masks\n",
            "predicted label = 1; gold label = 0\n",
            "       tokens     score\n",
            "0        good -0.038277\n",
            "1     morning  0.031962\n",
            "2     friends  0.012230\n",
            "3           !  0.256023\n",
            "4             -0.013876\n",
            "5           i  0.006017\n",
            "6        have -0.006208\n",
            "7           a  0.012426\n",
            "8    question -0.092443\n",
            "9           .  0.019244\n",
            "10         no  0.130987\n",
            "11  judgement -0.077967\n",
            "12          ,  0.057872\n",
            "13          i  0.005470\n",
            "14    promise -0.002789\n",
            "15          . -0.010885\n",
            "16            -0.007511\n",
            "17        are -0.000476\n",
            "18        you  0.015591\n",
            "19      still -0.027247\n",
            "20    wearing -0.022267\n",
            "21       your  0.001715\n",
            "22       mask -0.017866\n",
            "23       when -0.014147\n",
            "24    indoors -0.011763\n",
            "25   shopping -0.009971\n",
            "26          , -0.011456\n",
            "27         or -0.058106\n",
            "28         at -0.033474\n",
            "29        the -0.021002\n",
            "30       mall -0.022245\n",
            "31          ? -0.028404\n",
            "32            -0.018494\n",
            "33          #  0.008864\n",
            "34      <unk>  0.000000\n",
            "35          #  0.011885\n",
            "36      masks -0.082932\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-863423a82d204c5794177c1e265bff23\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-863423a82d204c5794177c1e265bff23\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-863423a82d204c5794177c1e265bff23\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-4628f6adadf34dbd0900b9cdc7a07e2f\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"good\", \"morning\", \"friends\", \"!\", \" \", \"i\", \"have\", \"a\", \"question\", \".\", \"no\", \"judgement\", \",\", \"i\", \"promise\", \".\", \" \", \"are\", \"you\", \"still\", \"wearing\", \"your\", \"mask\", \"when\", \"indoors\", \"shopping\", \",\", \"or\", \"at\", \"the\", \"mall\", \"?\", \" \", \"#\", \"<unk>\", \"#\", \"masks\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-4628f6adadf34dbd0900b9cdc7a07e2f\": [{\"tokens\": \"good\", \"score\": -0.038277240261770484}, {\"tokens\": \"morning\", \"score\": 0.0319615370322918}, {\"tokens\": \"friends\", \"score\": 0.012229581509575373}, {\"tokens\": \"!\", \"score\": 0.2560234567272959}, {\"tokens\": \" \", \"score\": -0.013876056441444217}, {\"tokens\": \"i\", \"score\": 0.006017002157542542}, {\"tokens\": \"have\", \"score\": -0.006207952955215434}, {\"tokens\": \"a\", \"score\": 0.012426352612921765}, {\"tokens\": \"question\", \"score\": -0.09244324343846347}, {\"tokens\": \".\", \"score\": 0.01924431837430207}, {\"tokens\": \"no\", \"score\": 0.13098739694887637}, {\"tokens\": \"judgement\", \"score\": -0.07796732241613126}, {\"tokens\": \",\", \"score\": 0.05787167240812461}, {\"tokens\": \"i\", \"score\": 0.005469669566323054}, {\"tokens\": \"promise\", \"score\": -0.0027892695650694505}, {\"tokens\": \".\", \"score\": -0.01088531475690747}, {\"tokens\": \" \", \"score\": -0.007510880613740787}, {\"tokens\": \"are\", \"score\": -0.0004763696335848949}, {\"tokens\": \"you\", \"score\": 0.015591330371145969}, {\"tokens\": \"still\", \"score\": -0.027247164056060588}, {\"tokens\": \"wearing\", \"score\": -0.022266997120739554}, {\"tokens\": \"your\", \"score\": 0.0017147515945772063}, {\"tokens\": \"mask\", \"score\": -0.0178655028841107}, {\"tokens\": \"when\", \"score\": -0.014146626035958445}, {\"tokens\": \"indoors\", \"score\": -0.011763285481963842}, {\"tokens\": \"shopping\", \"score\": -0.00997063133452595}, {\"tokens\": \",\", \"score\": -0.011456376286641996}, {\"tokens\": \"or\", \"score\": -0.05810635011765226}, {\"tokens\": \"at\", \"score\": -0.03347392107575964}, {\"tokens\": \"the\", \"score\": -0.021002199926305976}, {\"tokens\": \"mall\", \"score\": -0.02224468594899114}, {\"tokens\": \"?\", \"score\": -0.02840391249901689}, {\"tokens\": \" \", \"score\": -0.01849431975475918}, {\"tokens\": \"#\", \"score\": 0.008863728586311924}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.01188521342389331}, {\"tokens\": \"masks\", \"score\": -0.08293211777492997}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"Do what you want. But, I personally believe we should still be wearing #masks in public settings. Especially very crowded indoor public areas. For the time being at least. Who's with me? #Ottawa #Ontario #MaskUp #WearAMask #MaskMandate #COVID19 #COVID #CovidIsNotOver #DougFord\"\n",
        "df, tokens = interpret_sentence(tweet, true_label = 0.0)\n",
        "print(df)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iCnwpyRqRboz",
        "outputId": "de3a5297-ae33-40a7-989f-75c765fa47ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Do what you want. But, I personally believe we should still be wearing #masks in public settings. Especially very crowded indoor public areas. For the time being at least. Who's with me? #Ottawa #Ontario #MaskUp #WearAMask #MaskMandate #COVID19 #COVID #CovidIsNotOver #DougFord\n",
            "predicted label = 0; gold label = 0\n",
            "        tokens     score\n",
            "0           do  0.023612\n",
            "1         what  0.006425\n",
            "2          you  0.010153\n",
            "3         want  0.008118\n",
            "4            . -0.007561\n",
            "5          but  0.025619\n",
            "6            , -0.018537\n",
            "7            i -0.096060\n",
            "8   personally -0.086140\n",
            "9      believe -0.075975\n",
            "10          we -0.070131\n",
            "11      should -0.010968\n",
            "12       still -0.003058\n",
            "13          be -0.025509\n",
            "14     wearing  0.037614\n",
            "15           # -0.019107\n",
            "16       masks  0.018792\n",
            "17          in -0.013538\n",
            "18      public  0.055958\n",
            "19    settings  0.175334\n",
            "20           . -0.103784\n",
            "21  especially -0.027063\n",
            "22        very -0.023579\n",
            "23     crowded  0.010533\n",
            "24      indoor -0.018714\n",
            "25      public  0.000236\n",
            "26       areas -0.008532\n",
            "27           . -0.011056\n",
            "28         for  0.004916\n",
            "29         the  0.008533\n",
            "30        time  0.008761\n",
            "31       being  0.011456\n",
            "32          at  0.024069\n",
            "33       least -0.010108\n",
            "34           . -0.001592\n",
            "35         who  0.014947\n",
            "36          's  0.009921\n",
            "37        with  0.006263\n",
            "38          me  0.009640\n",
            "39           ?  0.079444\n",
            "40           #  0.021586\n",
            "41      ottawa  0.016968\n",
            "42           # -0.000611\n",
            "43     ontario -0.034907\n",
            "44           # -0.021604\n",
            "45       <unk>  0.000000\n",
            "46           # -0.022915\n",
            "47       <unk>  0.000000\n",
            "48           # -0.018984\n",
            "49       <unk>  0.000000\n",
            "50           # -0.017508\n",
            "51       <unk>  0.000000\n",
            "52           # -0.015993\n",
            "53       <unk>  0.000000\n",
            "54           # -0.021627\n",
            "55       <unk>  0.000000\n",
            "56           # -0.052655\n",
            "57       <unk>  0.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-d895cf9b037349559774a181c8b7f463\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-d895cf9b037349559774a181c8b7f463\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-d895cf9b037349559774a181c8b7f463\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a70bb2da5c043160f9290396d4a3f310\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"do\", \"what\", \"you\", \"want\", \".\", \"but\", \",\", \"i\", \"personally\", \"believe\", \"we\", \"should\", \"still\", \"be\", \"wearing\", \"#\", \"masks\", \"in\", \"public\", \"settings\", \".\", \"especially\", \"very\", \"crowded\", \"indoor\", \"public\", \"areas\", \".\", \"for\", \"the\", \"time\", \"being\", \"at\", \"least\", \".\", \"who\", \"'s\", \"with\", \"me\", \"?\", \"#\", \"ottawa\", \"#\", \"ontario\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a70bb2da5c043160f9290396d4a3f310\": [{\"tokens\": \"do\", \"score\": 0.02361180997846803}, {\"tokens\": \"what\", \"score\": 0.006425363473470582}, {\"tokens\": \"you\", \"score\": 0.010153188156351948}, {\"tokens\": \"want\", \"score\": 0.008118054486243895}, {\"tokens\": \".\", \"score\": -0.0075612048640379265}, {\"tokens\": \"but\", \"score\": 0.02561880114983658}, {\"tokens\": \",\", \"score\": -0.018537201932485384}, {\"tokens\": \"i\", \"score\": -0.0960601515908794}, {\"tokens\": \"personally\", \"score\": -0.08614021657723324}, {\"tokens\": \"believe\", \"score\": -0.07597494320370488}, {\"tokens\": \"we\", \"score\": -0.07013144490687537}, {\"tokens\": \"should\", \"score\": -0.010968052939674145}, {\"tokens\": \"still\", \"score\": -0.003058320553709156}, {\"tokens\": \"be\", \"score\": -0.025509104563158275}, {\"tokens\": \"wearing\", \"score\": 0.03761435108553139}, {\"tokens\": \"#\", \"score\": -0.019106728355925365}, {\"tokens\": \"masks\", \"score\": 0.018792259173313678}, {\"tokens\": \"in\", \"score\": -0.013538401157882123}, {\"tokens\": \"public\", \"score\": 0.055958189543194}, {\"tokens\": \"settings\", \"score\": 0.1753341900597314}, {\"tokens\": \".\", \"score\": -0.10378370402272089}, {\"tokens\": \"especially\", \"score\": -0.02706319588243974}, {\"tokens\": \"very\", \"score\": -0.023578596759234922}, {\"tokens\": \"crowded\", \"score\": 0.010532731585298223}, {\"tokens\": \"indoor\", \"score\": -0.018714254589720397}, {\"tokens\": \"public\", \"score\": 0.0002359575269690495}, {\"tokens\": \"areas\", \"score\": -0.008531994302538454}, {\"tokens\": \".\", \"score\": -0.011056452500279312}, {\"tokens\": \"for\", \"score\": 0.004916317054573968}, {\"tokens\": \"the\", \"score\": 0.008533261982661855}, {\"tokens\": \"time\", \"score\": 0.008761275380857591}, {\"tokens\": \"being\", \"score\": 0.011456447835216458}, {\"tokens\": \"at\", \"score\": 0.024068512870925318}, {\"tokens\": \"least\", \"score\": -0.010107720695925963}, {\"tokens\": \".\", \"score\": -0.0015919526989670435}, {\"tokens\": \"who\", \"score\": 0.014946624750964239}, {\"tokens\": \"'s\", \"score\": 0.009921202693769554}, {\"tokens\": \"with\", \"score\": 0.00626250883361766}, {\"tokens\": \"me\", \"score\": 0.009640369290432108}, {\"tokens\": \"?\", \"score\": 0.07944424565342065}, {\"tokens\": \"#\", \"score\": 0.0215862261652896}, {\"tokens\": \"ottawa\", \"score\": 0.0169675604036902}, {\"tokens\": \"#\", \"score\": -0.000610683771446401}, {\"tokens\": \"ontario\", \"score\": -0.03490700890198788}, {\"tokens\": \"#\", \"score\": -0.02160414271103367}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.022915008470638584}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.018983678871947237}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.017507507624250823}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.01599296792481947}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.021627299001287795}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.052655375749684895}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"Dozens and dozens and dozens of young sportspeople dying of heart related issues https://t.co/JZDZsYkDAw #VaccineSideEffects #death\"\n",
        "df, tokens = interpret_sentence(tweet, true_label = 2.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "TF4VdxKsSCCZ",
        "outputId": "eb5a595d-0898-48b8-e080-c7faa1bee53e"
      },
      "execution_count": 446,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Dozens and dozens and dozens of young sportspeople dying of heart related issues https://t.co/JZDZsYkDAw #VaccineSideEffects #death\n",
            "predicted label = 2; gold label = 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-10177148cee54b5fb86bad924a1739b5\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-10177148cee54b5fb86bad924a1739b5\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-10177148cee54b5fb86bad924a1739b5\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-c15c8ce7a06c8587b45577cb78d3ad8d\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"dozens\", \"and\", \"dozens\", \"and\", \"dozens\", \"of\", \"young\", \"<unk>\", \"dying\", \"of\", \"heart\", \"related\", \"issues\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"death\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-c15c8ce7a06c8587b45577cb78d3ad8d\": [{\"tokens\": \"dozens\", \"score\": 0.00526959577535792}, {\"tokens\": \"and\", \"score\": 7.109903305315047e-05}, {\"tokens\": \"dozens\", \"score\": 0.0013582916047535551}, {\"tokens\": \"and\", \"score\": -0.0005022249526863799}, {\"tokens\": \"dozens\", \"score\": 0.0017823137450291915}, {\"tokens\": \"of\", \"score\": 0.0005744262963295017}, {\"tokens\": \"young\", \"score\": 0.0007136236280519919}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"dying\", \"score\": 0.0018948106646834875}, {\"tokens\": \"of\", \"score\": -0.000474789666891538}, {\"tokens\": \"heart\", \"score\": -0.0012686370101025542}, {\"tokens\": \"related\", \"score\": -0.0009314524485257698}, {\"tokens\": \"issues\", \"score\": 0.002227157307559842}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.0005133705375405344}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.0003290397111064406}, {\"tokens\": \"death\", \"score\": 0.005211418271998323}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 446
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=\"Because only a healthy you can make the world beautiful and better too. Be healthy, keep well! #worldhealthday #health #life #betterlife #pandemic #covid #globalhealth #lockdown #medicine #vaccine #greatjob #warriors #goodhealth #healthday https://t.co/qn8X8pOJFo\"\n",
        "df, tokens = interpret_sentence(tweet, true_label = 1.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "FnIepTMSUwlB",
        "outputId": "26672d5c-9100-4b05-85a9-e5084bce633c"
      },
      "execution_count": 447,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Because only a healthy you can make the world beautiful and better too. Be healthy, keep well! #worldhealthday #health #life #betterlife #pandemic #covid #globalhealth #lockdown #medicine #vaccine #greatjob #warriors #goodhealth #healthday https://t.co/qn8X8pOJFo\n",
            "predicted label = 1; gold label = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-440cdd9bb49841f3905099cd56c23257\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-440cdd9bb49841f3905099cd56c23257\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-440cdd9bb49841f3905099cd56c23257\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-3e22a9e1c99207baeffc828b92d707bc\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"because\", \"only\", \"a\", \"healthy\", \"you\", \"can\", \"make\", \"the\", \"world\", \"beautiful\", \"and\", \"better\", \"too\", \".\", \"be\", \"healthy\", \",\", \"keep\", \"well\", \"!\", \"#\", \"<unk>\", \"#\", \"health\", \"#\", \"life\", \"#\", \"<unk>\", \"#\", \"pandemic\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"#\", \"lockdown\", \"#\", \"medicine\", \"#\", \"vaccine\", \"#\", \"<unk>\", \"#\", \"warriors\", \"#\", \"<unk>\", \"#\", \"<unk>\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-3e22a9e1c99207baeffc828b92d707bc\": [{\"tokens\": \"because\", \"score\": -0.013242116132872852}, {\"tokens\": \"only\", \"score\": -0.009369519086226625}, {\"tokens\": \"a\", \"score\": 0.003930429152080813}, {\"tokens\": \"healthy\", \"score\": 0.0022368492337898763}, {\"tokens\": \"you\", \"score\": -0.0059510280864296244}, {\"tokens\": \"can\", \"score\": 0.014000134462908799}, {\"tokens\": \"make\", \"score\": 0.010012395391446985}, {\"tokens\": \"the\", \"score\": 0.04896225312838401}, {\"tokens\": \"world\", \"score\": 0.06638035640503623}, {\"tokens\": \"beautiful\", \"score\": 0.3263504873627711}, {\"tokens\": \"and\", \"score\": 0.025290645493762812}, {\"tokens\": \"better\", \"score\": 0.040296706115647236}, {\"tokens\": \"too\", \"score\": -0.007203099696414083}, {\"tokens\": \".\", \"score\": 0.012385796278249449}, {\"tokens\": \"be\", \"score\": -0.0014623820430659893}, {\"tokens\": \"healthy\", \"score\": 0.00015593781025194825}, {\"tokens\": \",\", \"score\": 0.011578757732433172}, {\"tokens\": \"keep\", \"score\": 0.003832780360060339}, {\"tokens\": \"well\", \"score\": -0.005507431033908804}, {\"tokens\": \"!\", \"score\": 0.086422705693982}, {\"tokens\": \"#\", \"score\": 0.01043928249025307}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.001510618979765943}, {\"tokens\": \"health\", \"score\": -0.018484080831790164}, {\"tokens\": \"#\", \"score\": 0.0047703654192729735}, {\"tokens\": \"life\", \"score\": -0.008464113742566191}, {\"tokens\": \"#\", \"score\": 0.0092235028109928}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.0026102122621034568}, {\"tokens\": \"pandemic\", \"score\": -0.016621182085624152}, {\"tokens\": \"#\", \"score\": 0.010371463576543664}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.004039043851226582}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": -0.0008941783435083461}, {\"tokens\": \"lockdown\", \"score\": -0.011006702910730337}, {\"tokens\": \"#\", \"score\": 0.003020063040911319}, {\"tokens\": \"medicine\", \"score\": -0.008218882117151136}, {\"tokens\": \"#\", \"score\": -0.010796000838108616}, {\"tokens\": \"vaccine\", \"score\": -0.06516627383045011}, {\"tokens\": \"#\", \"score\": 0.007712622738200064}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.004191195812590036}, {\"tokens\": \"warriors\", \"score\": -0.0028910830856757187}, {\"tokens\": \"#\", \"score\": 0.003238793724106915}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"#\", \"score\": 0.010740388023401768}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 447
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"Triple vaxxed. Got COVID. Not in ICU. Thank you, vaccination! #VaccinesWork https://t.co/c0ldCCnn7s\"\n",
        "df, tokens = interpret_sentence(tweet, true_label = 1.0)\n",
        "visualisation(df, tokens, w=1100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "iR0meEDTTPyy",
        "outputId": "108892f1-879d-419e-834d-4a2672ca1b1f"
      },
      "execution_count": 460,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence = Triple vaxxed. Got COVID. Not in ICU. Thank you, vaccination! #VaccinesWork https://t.co/c0ldCCnn7s\n",
            "predicted label = 1; gold label = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-593d2f6230e14d449439a017f3bc942a\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-593d2f6230e14d449439a017f3bc942a\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-593d2f6230e14d449439a017f3bc942a\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-9ce29135283df8d0f0a94af427775118\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"score\", \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"tokens\", \"sort\": [\"triple\", \"<unk>\", \".\", \"got\", \"<unk>\", \".\", \"not\", \"in\", \"icu\", \".\", \"thank\", \"you\", \",\", \"<unk>\", \"!\", \"#\", \"<unk>\", \"<unk>\"], \"type\": \"nominal\"}}, \"height\": 70, \"width\": 1100, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-9ce29135283df8d0f0a94af427775118\": [{\"tokens\": \"triple\", \"score\": 0.07198386998264067}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \".\", \"score\": 0.02595727144701611}, {\"tokens\": \"got\", \"score\": 0.019039115056904938}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \".\", \"score\": 0.05239742033202124}, {\"tokens\": \"not\", \"score\": -0.15077109353071383}, {\"tokens\": \"in\", \"score\": -0.05894463571307143}, {\"tokens\": \"icu\", \"score\": -0.09844592107107361}, {\"tokens\": \".\", \"score\": 0.23140133604910856}, {\"tokens\": \"thank\", \"score\": 0.41939915641238085}, {\"tokens\": \"you\", \"score\": 0.05403993859445329}, {\"tokens\": \",\", \"score\": 0.03389909860673322}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"!\", \"score\": 0.23814824366745194}, {\"tokens\": \"#\", \"score\": 0.13787488505936785}, {\"tokens\": \"<unk>\", \"score\": 0.0}, {\"tokens\": \"<unk>\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 460
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wXN1TfrQV_oc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}