{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bertweet_emoji.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f4kXTKqY9oWq",
        "3TQwj25S9ur8",
        "s2YdLQbA-RcD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bertweet model for sentiment with intermediate fine-tuning on emoji prediction"
      ],
      "metadata": {
        "id": "s_kQr-sd9e_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. install pakages"
      ],
      "metadata": {
        "id": "f4kXTKqY9oWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "birc52jYYu23",
        "outputId": "af771fa9-af10-45e2-d242-dfa9dec9b368"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5yKfW9LzgUcE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.optimization import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6IT1s19Y-raU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56646770-b14d-4d89-9e93-e28b83cd59dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kf01nlmYJUW",
        "outputId": "20e113f4-95ca-4527-d5c6-8486e8a8c27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "# For transformers v4.x+:\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lRDUTZ8klGUU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. data preparation "
      ],
      "metadata": {
        "id": "3TQwj25S9ur8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# define a function for data preparation\n",
        "def data_prepare(file_path, lab2ind, tokenizer, max_len = 64, mode = 'train'):\n",
        "    '''\n",
        "    file_path: the path to input file. \n",
        "                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n",
        "                The first row must be header of columns.\n",
        "\n",
        "                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n",
        "                The first row must be header of column.\n",
        "\n",
        "    lab2ind: dictionary of label classes\n",
        "    tokenizer: BERT tokenizer\n",
        "    max_len: maximal length of input sequence\n",
        "    mode: train or predict\n",
        "    '''\n",
        "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
        "    if mode == 'train':\n",
        "        # Use pandas to load dataset\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content','label'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        labels = df.label.values\n",
        "        \n",
        "        # Create sentence and label lists\n",
        "        labels = [lab2ind[i] for i in labels] \n",
        "        print(\"Label is \", labels[0])\n",
        "        \n",
        "        # Convert data into torch tensors\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # if we are in predict mode, we will load one column (i.e., text).\n",
        "    elif mode == 'predict':\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        # create placeholder\n",
        "        labels = []\n",
        "    else:\n",
        "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
        "        return\n",
        "        \n",
        "    # Create sentence and label lists\n",
        "    content = df.content.values\n",
        "    #### REF START ####\n",
        "\n",
        "    # We need to add a special token at the beginning for BERT to work properly.\n",
        "    content = [\"[CLS] \" + text for text in content]\n",
        "\n",
        "    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n",
        "    \n",
        "    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n",
        "    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n",
        "\n",
        "    # We also need to add a special token at the end.\n",
        "    tokenized_texts = [ text+['[SEP]'] for text in tokenized_texts]\n",
        "    print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
        "    \n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
        "\n",
        "    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "    print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for pad tokens\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Convert all of our data into torch tensors, the required datatype for our model\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "    #### REF END ####\n",
        "\n",
        "    return inputs, labels, masks    "
      ],
      "metadata": {
        "id": "C89M1ukabDcP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab2ind_emoji = {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 11:11, 12:12, 13:13, 14:14, 15:15, 16:16, 17:17, 18:18, 19:19}"
      ],
      "metadata": {
        "id": "Y48OUfVWIqLL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab2ind_sentiment = {0: 0, 1: 1, 2:2}"
      ],
      "metadata": {
        "id": "95zz4t1Bgl3q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs_emoji, train_labels_emoji, train_masks_emoji = data_prepare(\"train_emoji.tsv\", lab2ind_emoji,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeaKGhImihdm",
        "outputId": "69c4406c-0612-490a-fd23-3277016e5aa9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (45000, 2)\n",
            "Label is  12\n",
            "Tokenize the first sentence:\n",
            " ['[@@', 'CL@@', 'S@@', ']', 'Sunday', 'afternoon', 'walking', 'through', 'Venice', 'in', 'the', 'sun', 'with', '@@@', 'user', '️', '️', '️', '@', 'Ab@@', 'bot', 'Kin@@', 'ney@@', ',', 'Venice', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [61658, 6411, 381, 317, 970, 2464, 1508, 292, 18911, 16, 6, 1599, 30, 5238, 4699, 3, 3, 3, 59157, 3547, 5178, 12229, 11558, 7, 18911, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [61658  6411   381   317   970  2464  1508   292 18911    16     6  1599\n",
            "    30  5238  4699     3     3     3 59157  3547  5178 12229 11558     7\n",
            " 18911     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_inputs_emoji, validation_labels_emoji, validation_masks_emoji = data_prepare(\"val_emoji.tsv\", lab2ind_emoji,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdVbOc6FobIj",
        "outputId": "7a91f0dd-ed93-493c-f97b-331552340373"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (5000, 2)\n",
            "Label is  0\n",
            "Tokenize the first sentence:\n",
            " ['[@@', 'CL@@', 'S@@', ']', 'A', 'little', 'throwback', 'with', 'my', 'favourite', 'person', '@', 'Water', 'Wall', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [61658, 6411, 381, 317, 104, 263, 16189, 30, 23, 2060, 282, 59157, 3782, 3072, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [61658  6411   381   317   104   263 16189    30    23  2060   282 59157\n",
            "  3782  3072     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "train_data_emoji = TensorDataset(train_inputs_emoji, train_masks_emoji, train_labels_emoji)\n",
        "train_dataloader_emoji = DataLoader(train_data_emoji, \n",
        "                              sampler = RandomSampler(train_data_emoji), # Select batches randomly\n",
        "                              batch_size=batch_size)"
      ],
      "metadata": {
        "id": "zEJ3Az8dliVj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data_emoji = TensorDataset(validation_inputs_emoji, validation_masks_emoji, validation_labels_emoji)\n",
        "validation_dataloader_emoji = DataLoader(validation_data_emoji, \n",
        "                                   sampler = SequentialSampler(validation_data_emoji), # Pull out batches sequentially.\n",
        "                                   batch_size=batch_size)"
      ],
      "metadata": {
        "id": "C7f40nyfoeNG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs_sentiment, train_labels_sentiment, train_masks_sentiment = data_prepare(\"train_sentiment.tsv\", lab2ind_sentiment,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_dKbQX5h7Fn",
        "outputId": "5b0dcb80-c768-4b47-a153-e269dea3b949"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (45615, 2)\n",
            "Label is  2\n",
            "Tokenize the first sentence:\n",
            " ['[@@', 'CL@@', 'S@@', ']', '\"@@', 'QT', '@@@', 'user', 'In', 'the', 'original', 'draft', 'of', 'the', '7th', 'book@@', ',', 'Rem@@', 'us', 'Lup@@', 'in', 'survived', 'the', 'Battle', 'of', 'Hog@@', 'war@@', 'ts@@', '.', '#HappyBirthday@@', 'Rem@@', 'us@@', 'Lup@@', 'in@@', '\"', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [61658, 6411, 381, 317, 61933, 19989, 5238, 4699, 173, 6, 1782, 4271, 15, 6, 4133, 8470, 7, 8867, 148, 30411, 16, 9610, 6, 2606, 15, 60201, 3628, 4813, 4, 11835, 8867, 1924, 30411, 520, 26, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [61658  6411   381   317 61933 19989  5238  4699   173     6  1782  4271\n",
            "    15     6  4133  8470     7  8867   148 30411    16  9610     6  2606\n",
            "    15 60201  3628  4813     4 11835  8867  1924 30411   520    26     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_inputs_sentiment, validation_labels_sentiment, validation_masks_sentiment = data_prepare(\"val_sentiment.tsv\", lab2ind_sentiment,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg6-cj44iPIn",
        "outputId": "912b37e1-540c-4dfc-ae14-71dfed434da8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (2000, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[@@', 'CL@@', 'S@@', ']', 'Dark', 'Souls', '3', 'April', 'Launch', 'Date', 'Confirmed', 'With', 'New', 'Trail@@', 'er:', 'Embrace', 'the', 'dark@@', 'ness@@', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [61658, 6411, 381, 317, 3713, 20987, 163, 1249, 9767, 2553, 22650, 458, 210, 53681, 25570, 26755, 6, 13863, 16571, 4, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [61658  6411   381   317  3713 20987   163  1249  9767  2553 22650   458\n",
            "   210 53681 25570 26755     6 13863 16571     4     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "train_data_sentiment = TensorDataset(train_inputs_sentiment, train_masks_sentiment, train_labels_sentiment)\n",
        "train_dataloader_sentiment = DataLoader(train_data_sentiment, \n",
        "                              sampler = RandomSampler(train_data_sentiment), # Select batches randomly\n",
        "                              batch_size=batch_size)"
      ],
      "metadata": {
        "id": "OLJMwR_fiZdH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data_sentiment = TensorDataset(validation_inputs_sentiment, validation_masks_sentiment, validation_labels_sentiment)\n",
        "validation_dataloader_sentiment = DataLoader(validation_data_sentiment, \n",
        "                                   sampler = SequentialSampler(validation_data_sentiment), # Pull out batches sequentially.\n",
        "                                   batch_size=batch_size)"
      ],
      "metadata": {
        "id": "McR0Z5YJicmD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Bertweet model - emoji"
      ],
      "metadata": {
        "id": "s2YdLQbA-RcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bertweet_cls(nn.Module):\n",
        "\n",
        "    def __init__(self, lab2ind, model_path, hidden_size):\n",
        "        super(Bertweet_cls, self).__init__()\n",
        "        self.model_path = model_path\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bert_model = AutoModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True)\n",
        "        \n",
        "        self.label_num = len(lab2ind)\n",
        "        \n",
        "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.label_num)\n",
        "\n",
        "    def forward(self, bert_ids, bert_mask):\n",
        "        outputs = self.bert_model(input_ids=bert_ids, attention_mask = bert_mask)\n",
        "        pooler_output = outputs['pooler_output']\n",
        "        attentions = outputs['attentions']\n",
        "        \n",
        "        x = self.dense(pooler_output)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        fc_output = self.fc(x)\n",
        "\n",
        "        return fc_output, attentions"
      ],
      "metadata": {
        "id": "GzRXvw6mkUtJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model = Bertweet_cls(lab2ind_emoji, \"vinai/bertweet-base\", 768).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrZwMjJnk5LL",
        "outputId": "2ba4171a-8ac3-453c-e5f1-8735f5f852d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for emoji model:\n",
        "lr = 0.0001\n",
        "max_grad_norm = 1.0\n",
        "epochs = 5\n",
        "warmup_proportion = 0.1\n",
        "num_training_steps  = len(train_dataloader_emoji) * epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion\n",
        "\n",
        "### In Transformers, optimizer and schedules are instantiated like this:\n",
        "# Note: AdamW is a class from the huggingface library\n",
        "# the 'W' stands for 'Weight Decay\"\n",
        "optimizer = AdamW(bertweet_model.parameters(), lr=lr, correct_bias=False)\n",
        "# schedules\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "# We use nn.CrossEntropyLoss() as our loss function. \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfP-2aVUlMIY",
        "outputId": "1d5fc9b8-5333-413e-9f99-cbf1f0908ac8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. model training - emoji\n"
      ],
      "metadata": {
        "id": "AtoL4V0b_BsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        outputs,_ = model(input_ids, input_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    # free GPU memory\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "6FC4D8HIloFj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            # Unpack the inputs from our dataloader\n",
        "            input_ids, input_mask, labels = batch\n",
        "\n",
        "            outputs,_ = model(input_ids, input_mask)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # delete used variables to free GPU memory\n",
        "            del batch, input_ids, input_mask\n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score"
      ],
      "metadata": {
        "id": "H4ENXtUpnghk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "save_path = './drive/My Drive/Colab Notebooks/ckpt_BERTweet/'\n",
        "if os.path.exists(save_path) == False:\n",
        "    os.makedirs(save_path)"
      ],
      "metadata": {
        "id": "wFF07Afbnhzn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss = train(bertweet_model, train_dataloader_emoji, optimizer, scheduler, criterion)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(bertweet_model, validation_dataloader_emoji, criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': bertweet_model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "        }\n",
        "\n",
        "    torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_BERTweet/BERT_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss, val_acc, val_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-L71Sd7nvYy",
        "outputId": "bef3dd17-495d-4ff9-ef66-3fad31c39bcf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  20%|██        | 1/5 [18:07<1:12:30, 1087.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/5], Train Loss: 2.1043, Validation Loss: 2.3254, Validation Accuracy: 0.2946, Validation F1: 0.2040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [36:03<54:02, 1080.87s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [2/5], Train Loss: 1.7357, Validation Loss: 2.3789, Validation Accuracy: 0.2930, Validation F1: 0.2387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [54:02<35:59, 1079.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [3/5], Train Loss: 1.4122, Validation Loss: 2.3885, Validation Accuracy: 0.3122, Validation F1: 0.2636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [1:12:02<17:59, 1079.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [4/5], Train Loss: 1.0733, Validation Loss: 2.6220, Validation Accuracy: 0.3108, Validation F1: 0.2828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 5/5 [1:30:02<00:00, 1080.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [5/5], Train Loss: 0.7778, Validation Loss: 2.8473, Validation Accuracy: 0.3096, Validation F1: 0.2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_emoji = torch.load(\"./drive/My Drive/Colab Notebooks/ckpt_BERTweet/BERT_5.pt\",map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "b405P-F8tefS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize weight for the linear layer\n",
        "best_emoji['state_dict']['fc.weight']= torch.randn([3, 768]) #need to be normal;"
      ],
      "metadata": {
        "id": "qFxZpW-rvFmC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_emoji['state_dict']['fc.bias']=torch.zeros([3]) #all 0s for bias"
      ],
      "metadata": {
        "id": "8COvNOQ-YSdu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_emoji, \"./drive/My Drive/Colab Notebooks/ckpt_BERTweet/best_emoji\"+\".pt\")"
      ],
      "metadata": {
        "id": "gIQNUXjivlqy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Bertweet model - sentiment"
      ],
      "metadata": {
        "id": "Mq7UEU46_X8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_emoji_checkpoint = torch.load(\"./drive/My Drive/Colab Notebooks/ckpt_BERTweet/best_emoji.pt\",map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "tqYazdXfkVxQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Bertweet_cls(lab2ind_sentiment, \"vinai/bertweet-base\", 768).to(device)\n",
        "model.load_state_dict(best_emoji_checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGERlHWd5ioH",
        "outputId": "3f922504-3553-4ffa-cfbc-6c59f20d6100"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for sentiment:\n",
        "lr = 0.00002\n",
        "max_grad_norm = 1.0\n",
        "epochs = 3\n",
        "warmup_proportion = 0.1\n",
        "num_training_steps  = len(train_dataloader_sentiment) * epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion\n",
        "\n",
        "### In Transformers, optimizer and schedules are instantiated like this:\n",
        "# Note: AdamW is a class from the huggingface library\n",
        "# the 'W' stands for 'Weight Decay\"\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
        "# schedules\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "# We use nn.CrossEntropyLoss() as our loss function. \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2NJRr1hjyrQ",
        "outputId": "14e5f756-4be6-49d4-b267-90806eca49bb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. model training - sentiment"
      ],
      "metadata": {
        "id": "FDutZ1tX_g-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the sentiment model\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss = train(model, train_dataloader_sentiment, optimizer, scheduler, criterion)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(model, validation_dataloader_sentiment, criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "        }\n",
        "\n",
        "    torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_BERTweet/BERT_sentiment\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss, val_acc, val_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAyhr-v-hw85",
        "outputId": "a1865d0b-dd8a-4e01-ce0c-624e12126cb2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  33%|███▎      | 1/3 [17:48<35:37, 1068.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/3], Train Loss: 1.3196, Validation Loss: 0.6831, Validation Accuracy: 0.7195, Validation F1: 0.7051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [35:37<17:48, 1068.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [2/3], Train Loss: 0.5974, Validation Loss: 0.6597, Validation Accuracy: 0.7265, Validation F1: 0.7121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [53:27<00:00, 1069.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [3/3], Train Loss: 0.4628, Validation Loss: 0.6841, Validation Accuracy: 0.7340, Validation F1: 0.7139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "smaller learning rate for sentiment than for emoji bc it is more fine-grined."
      ],
      "metadata": {
        "id": "pX7iB6-KxTXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. model evaluation - sentiment"
      ],
      "metadata": {
        "id": "PZgAxBim_oaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs_sentiment, test_labels_sentiment, test_masks_sentiment = data_prepare(\"test_sentiment.tsv\", lab2ind_sentiment,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBIB0Y-MAUKE",
        "outputId": "0a0bf724-70ad-4024-dd68-e7df7193b0ed"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (12284, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[@@', 'CL@@', 'S@@', ']', '@@@', 'user', '@@@', 'user', 'what', 'do', 'these', \"'@@\", '1/2', 'naked', 'pic@@', 's@@', \"'\", 'have', 'to', 'do', 'with', 'anything@@', '?', 'They@@', \"'re\", 'not', 'even', 'like', 'that@@', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [61658, 6411, 381, 317, 5238, 4699, 5238, 4699, 66, 32, 198, 1909, 3300, 1842, 7781, 423, 69, 36, 9, 32, 30, 48735, 21, 32752, 81, 46, 132, 43, 6139, 4, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [61658  6411   381   317  5238  4699  5238  4699    66    32   198  1909\n",
            "  3300  1842  7781   423    69    36     9    32    30 48735    21 32752\n",
            "    81    46   132    43  6139     4     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "test_data_sentiment = TensorDataset(test_inputs_sentiment, test_masks_sentiment, test_labels_sentiment)\n",
        "test_dataloader_sentiment = DataLoader(test_data_sentiment, \n",
        "                              sampler = RandomSampler(test_data_sentiment), # Select batches randomly\n",
        "                              batch_size=batch_size)"
      ],
      "metadata": {
        "id": "nM2ZfFeZAeoY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_sentiment_checkpoint = torch.load(\"./drive/My Drive/Colab Notebooks/ckpt_BERTweet/BERT_sentiment3.pt\",map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "YsDnwm0MXOnK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sentiment = Bertweet_cls(lab2ind_sentiment, \"vinai/bertweet-base\", 768).to(device)\n",
        "model_sentiment.load_state_dict(best_sentiment_checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7eHB_URXkCG",
        "outputId": "298d464c-40a1-484d-e29e-de8ad8ed69eb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, test_f1 = evaluate(model_sentiment, test_dataloader_sentiment, criterion)\n",
        "print('Test Loss: {:.4f}, Test Accuracy: {:.4f}, Test F1: {:.4f}'.format(test_loss, test_acc, test_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBmOt--U_9iK",
        "outputId": "43ac088a-96de-42cb-e95b-54f110ae8639"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7273, Test Accuracy: 0.7044, Test F1: 0.7056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on vaccine data"
      ],
      "metadata": {
        "id": "88cDeOAYrwYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lab2ind_vac = {'negative': 0, 'neutral':1, 'positive':2}"
      ],
      "metadata": {
        "id": "sk3eFEZ_r4zH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vac_inputs_sentiment, vac_labels_sentiment, vac_masks_sentiment = data_prepare(\"test_vaccines.tsv\", lab2ind_vac,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5E6snBOrUaI",
        "outputId": "cee0dfae-41af-40af-c345-4f97482d642c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (101, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[@@', 'CL@@', 'S@@', ']', 'According', 'to', 'the', 'latest', '#v@@', 'acc@@', 'ine', 'report', 'by', 'the', '#UK@@', 'Health@@', 'Sec@@', 'urity@@', 'Ag@@', 'ency@@', ',', '#V@@', 'acc@@', 'ine@@', 'Eff@@', 'ectiveness', 'for', 'tri@@', 'ple-@@', 'jab@@', 'bed', 'young', 'and', 'older', 'people', 'are', 'below', 'zero@@', ',', 'and', 'more', 'likely', 'to', 'get', 'with', '#Co@@', 'vid', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [61658, 6411, 381, 317, 3877, 9, 6, 1405, 6615, 6530, 1466, 1649, 61, 6, 20461, 15251, 11060, 43243, 7709, 35615, 7, 2863, 6530, 3800, 24037, 61598, 19, 3203, 35939, 27819, 424, 859, 13, 1879, 83, 41, 2703, 46631, 7, 13, 89, 2056, 9, 51, 30, 8232, 4759, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [61658  6411   381   317  3877     9     6  1405  6615  6530  1466  1649\n",
            "    61     6 20461 15251 11060 43243  7709 35615     7  2863  6530  3800\n",
            " 24037 61598    19  3203 35939 27819   424   859    13  1879    83    41\n",
            "  2703 46631     7    13    89  2056     9    51    30  8232  4759     3\n",
            "     3     3     3     3     3     3     3     3     3     3     3     3\n",
            "     3     3     3     3     3     3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "vac_data_sentiment = TensorDataset(vac_inputs_sentiment, vac_masks_sentiment, vac_labels_sentiment)\n",
        "vac_dataloader_sentiment = DataLoader(vac_data_sentiment, \n",
        "                              sampler = RandomSampler(vac_data_sentiment), # Select batches randomly\n",
        "                              batch_size=batch_size)"
      ],
      "metadata": {
        "id": "4WbNbU1YsH3E"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_sentiment_checkpoint = torch.load(\"./drive/My Drive/Colab Notebooks/ckpt_BERTweet/BERT_sentiment3.pt\",map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "17OxIfQUsRwX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sentiment = Bertweet_cls(lab2ind_vac, \"vinai/bertweet-base\", 768).to(device)\n",
        "model_sentiment.load_state_dict(best_sentiment_checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_pwexxpsT9K",
        "outputId": "d9a9e9f4-0c86-4557-8576-df44edd0bb54"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "test_loss, test_acc, test_f1 = evaluate(model_sentiment, vac_dataloader_sentiment, criterion)\n",
        "print('Test Loss: {:.4f}, Test Accuracy: {:.4f}, Test F1: {:.4f}'.format(test_loss, test_acc, test_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvVU6MD1sa6k",
        "outputId": "694d81ba-d7d2-4ff8-8d70-ebda82a1f9b6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5339, Test Accuracy: 0.7624, Test F1: 0.7461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on masks"
      ],
      "metadata": {
        "id": "l9e0JS0hswVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_inputs_sentiment, mask_labels_sentiment, mask_masks_sentiment = data_prepare(\"test_masks.tsv\", lab2ind_vac,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnsLh17wsfyk",
        "outputId": "b4677802-f193-4be7-d35f-5bc77529ad58"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (90, 2)\n",
            "Label is  2\n",
            "Tokenize the first sentence:\n",
            " ['[@@', 'CL@@', 'S@@', ']', 'Thank', 'you', 'to', 'all', 'those', 'who', 'still', 'limit', 'indoor', 'social', 'gather@@', 'ings@@', ',', 'get', 'boo@@', 'sted@@', ',', 'mask', 'at', 'the', 'grocery', 'store', '&@@', 'amp@@', ';', 'send', 'their', 'children', 'to', 'school', 'in', 'mask@@', 's.', 'I', 'see', 'you', 'and', 'appreciate', 'you', 'caring', 'about', 'the', 'safety', 'of', 'the', 'rest', 'of', 'us@@', '.', 'We', 'over', 'me@@', '.', '#COVID19', '#Covid@@', 'Is@@', 'Not@@', 'Over', '#M@@', 'ask@@', 'Up', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [61658, 6411, 381, 317, 396, 14, 9, 48, 268, 87, 135, 3947, 17617, 1009, 62780, 17726, 7, 51, 4910, 41756, 7, 7146, 35, 6, 8923, 1297, 19295, 6755, 208, 786, 130, 994, 9, 230, 16, 61163, 32188, 8, 95, 14, 13, 1621, 14, 5325, 62, 6, 3705, 15, 6, 765, 15, 1924, 4, 134, 141, 1677, 4, 4270, 60605, 3166, 8719, 1773, 1230, 11686, 857, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [61658  6411   381   317   396    14     9    48   268    87   135  3947\n",
            " 17617  1009 62780 17726     7    51  4910 41756     7  7146    35     6\n",
            "  8923  1297 19295  6755   208   786   130   994     9   230    16 61163\n",
            " 32188     8    95    14    13  1621    14  5325    62     6  3705    15\n",
            "     6   765    15  1924     4   134   141  1677     4  4270 60605  3166\n",
            "  8719  1773  1230 11686   857     3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "mask_data_sentiment = TensorDataset(mask_inputs_sentiment, mask_masks_sentiment, mask_labels_sentiment)\n",
        "mask_dataloader_sentiment = DataLoader(mask_data_sentiment, \n",
        "                              sampler = RandomSampler(mask_data_sentiment), # Select batches randomly\n",
        "                              batch_size=batch_size)"
      ],
      "metadata": {
        "id": "bBtartIks9Hl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, test_f1 = evaluate(model_sentiment, mask_dataloader_sentiment, criterion)\n",
        "print('Test Loss: {:.4f}, Test Accuracy: {:.4f}, Test F1: {:.4f}'.format(test_loss, test_acc, test_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTFoEbWytGet",
        "outputId": "2b93717c-a8b8-4ae3-d1c9-48f4e15dbbb9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6645, Test Accuracy: 0.7222, Test F1: 0.6913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. predict "
      ],
      "metadata": {
        "id": "c2zst96Bw18B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sententce_prepocess(content, tokenizer, max_len = 64):\n",
        "    \"\"\"\n",
        "    content: list of string. Each string is a sample. We only include one sample in this list.\n",
        "    tokenizer: BertTokenizerFast\n",
        "    \"\"\"\n",
        "    #### REF START ####\n",
        "\n",
        "    # We need to add a special token at the beginning for BERT to work properly.\n",
        "    content = [\"[CLS] \" + text for text in content]\n",
        "\n",
        "    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n",
        "    \n",
        "    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n",
        "    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n",
        "\n",
        "    # We also need to add a special token at the end.\n",
        "    tokenized_texts = [ text+['[SEP]'] for text in tokenized_texts]\n",
        "    #print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
        "    \n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    #print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
        "\n",
        "    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "    #print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for pad tokens\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Convert all of our data into torch tensors, the required datatype for our model\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "    #### REF END ####\n",
        "\n",
        "    return tokenized_texts, inputs, masks"
      ],
      "metadata": {
        "id": "grqQYNWrw0uI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks_df = pd.read_csv('masks_twitter.csv')\n",
        "masks_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "DggxvH265zue",
        "outputId": "18b259b1-be8e-4e0f-ec22-fc5a392b5847"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             author_id created_at             tweet_id  \\\n",
              "0   835967581693612033    2021-07  1421619622445469697   \n",
              "1   825765275853344773    2021-07  1421619085876547586   \n",
              "2  1412770891335864320    2021-07  1421617357051596804   \n",
              "3  1254787933493575684    2021-07  1421617137991536642   \n",
              "4   841625519779139584    2021-07  1421612879791501312   \n",
              "\n",
              "                                          tweet_text  CNN_prediction  \\\n",
              "0  The manipulative, politically opportune lying ...             NaN   \n",
              "1  @ReallyAmerican1 Someone should introduce a bi...             NaN   \n",
              "2  Just wear a friggin #mask we will never #MakeA...             NaN   \n",
              "3  Proof Vaccines 💉 Do Not work but Harm\\n\\nCDC r...             NaN   \n",
              "4  @barali4793 @RadioFreeTom We now have a huge p...             NaN   \n",
              "\n",
              "   CNN_confidence  BERTweet_prediction  BERTweet_confidence  \\\n",
              "0             NaN                  NaN                  NaN   \n",
              "1             NaN                  NaN                  NaN   \n",
              "2             NaN                  NaN                  NaN   \n",
              "3             NaN                  NaN                  NaN   \n",
              "4             NaN                  NaN                  NaN   \n",
              "\n",
              "   BERTweet_fine-tuned_prediction  BERTweet_fine-tuned_confidence  \\\n",
              "0                             NaN                             NaN   \n",
              "1                             NaN                             NaN   \n",
              "2                             NaN                             NaN   \n",
              "3                             NaN                             NaN   \n",
              "4                             NaN                             NaN   \n",
              "\n",
              "   final_prediction  \n",
              "0               NaN  \n",
              "1               NaN  \n",
              "2               NaN  \n",
              "3               NaN  \n",
              "4               NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67ccb713-c585-4754-a069-211c418de031\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>CNN_prediction</th>\n",
              "      <th>CNN_confidence</th>\n",
              "      <th>BERTweet_prediction</th>\n",
              "      <th>BERTweet_confidence</th>\n",
              "      <th>BERTweet_fine-tuned_prediction</th>\n",
              "      <th>BERTweet_fine-tuned_confidence</th>\n",
              "      <th>final_prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>835967581693612033</td>\n",
              "      <td>2021-07</td>\n",
              "      <td>1421619622445469697</td>\n",
              "      <td>The manipulative, politically opportune lying ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>825765275853344773</td>\n",
              "      <td>2021-07</td>\n",
              "      <td>1421619085876547586</td>\n",
              "      <td>@ReallyAmerican1 Someone should introduce a bi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1412770891335864320</td>\n",
              "      <td>2021-07</td>\n",
              "      <td>1421617357051596804</td>\n",
              "      <td>Just wear a friggin #mask we will never #MakeA...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1254787933493575684</td>\n",
              "      <td>2021-07</td>\n",
              "      <td>1421617137991536642</td>\n",
              "      <td>Proof Vaccines 💉 Do Not work but Harm\\n\\nCDC r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>841625519779139584</td>\n",
              "      <td>2021-07</td>\n",
              "      <td>1421612879791501312</td>\n",
              "      <td>@barali4793 @RadioFreeTom We now have a huge p...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67ccb713-c585-4754-a069-211c418de031')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67ccb713-c585-4754-a069-211c418de031 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67ccb713-c585-4754-a069-211c418de031');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_result = []\n",
        "emoji_confid = []\n",
        "for index, row in masks_df.iterrows():\n",
        "    tokenized_texts, input_ids, masks = sententce_prepocess([row['tweet_text']], tokenizer)\n",
        "    input_ids, masks = input_ids.to(device), masks.to(device)\n",
        "    outputs,_ = model_sentiment(input_ids, masks)\n",
        "    probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "    emoji_result.append(predicted.item())\n",
        "    emoji_confid.append(probabilities.item())\n"
      ],
      "metadata": {
        "id": "yRGCUyL75_1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}